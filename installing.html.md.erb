---
title: Installing and Upgrading Redis for PCF
owner: London Services
---

## <a id="install"></a>Installation Steps

To add Redis for PCF to Ops Manager, follow the procedure for adding Pivotal Ops Manager tiles:

1. Download the product file from [Pivotal Network](https://network.pivotal.io/products/p-redis).
1. Upload the product file to your Ops Manager installation.
1. Click **Add** next to the uploaded product description in the Available Products view to add this product to your staging area.
1. (Optional) Click the newly added tile to configure your [possible service plans](#configuration), [syslog draining](#syslog), and [backups](#backup).
1. Click **Apply Changes** to install the service.

After installing, be sure to:

+ Monitor the health and performance of your Redis instances by setting up [logging] (https://docs.pivotal.io/redis/1-7/installing.html#syslog) and [tracking metrics] (https://docs.pivotal.io/redis/1-7/monitoring.html).
+ Configure [automatic backups] (https://docs.pivotal.io/redis/1-7/installing.html#backup).

## <a id="configuration"></a>Configuring PCF Redis

###  <a id="service-plans"></a>Configure Redis Service Plans

Select the **Redis** tile from the Installation Dashboard to display the configuration page and allocate resources to Redis service plans.

####  <a id="shared-vm-config"></a>Shared-VM Plan

1.  Select the **Shared-VM Plan** tab to configure the memory limit for each Redis instance and the maximum number of instances that can be created.
![shared vm config](shared-vm-config.png)

1.  Enter the maximum number of instances and the memory limit for each Redis instance.

1.  Click **Save**.

    Shared-VM instances run on the Redis broker VM, so 
    the memory and instance limits for instances of this plan depend on the total system memory of your Redis broker VM. 
    
    <br>Pivotal recommends you only allow up to 45% of the Redis broker VM's total system memory to be used by all shared-VM instances combined.
    This is due to the amount of memory required to support Redis persistence and to run Redis broker and system tasks.

    <br />To configure the limits in these fields, 
    estimate the maximum memory that could be used by all your Redis shared-VM instances combined. 
    If that figure is higher than 45% of the Redis broker 
    VM's total system memory, you can do one of the following:<br><br>
    <ul>
      <li>Decrease the **Redis Instance Memory Limit**.</li>
      <li>Decrease the number of instances in **Redis Service Instance Limit**.</li>
      <li>Increase the RAM for the Redis Broker in the **Resource Config** tab as shown below.
          <img src="./images/redis-broker-resource.png"></li>
    </ul>
    
    <br />See below for example cases:

    <table border="1" class="nice">
    	<tr>
    		<th>Redis Broker VM Total Memory</th>
    		<th>Redis Instance Memory Limit</th>
    		<th>Redis Service Instance Limit</th>
    	</tr>
    	<tr>
    	 	<td>16GB</td>
    	 	<td>512MB</td>
    	 	<td>14</td>
     	</tr>
      <tr>
    	 	<td>16GB</td>
    	 	<td>256MB</td>
    	 	<td>28</td>
     	</tr>
      <tr>
    	 	<td>64GB</td>
    	 	<td>512MB</td>
    	 	<td>56</td>
     	</tr>
    </table>


    <br />It is possible to configure a larger Redis Service Instance Limit, if you are confident that the majority of the deployed instances will not be using a large amount of their allocated memory, for example in development or test environments.
    <p class="note"><strong>Note</strong>:
      This is not supported, and could cause your server to run out of memory.
      If this happens your users may not be able to write any further data to any Redis instance.
    </p>


1.  Select the **Resource Config** tab to change the allocation of resources for the Redis Broker.
    <br />The Redis Broker server will run all of the Redis instances for your Shared-VM plan. From this screen you may increase or decrease the CPU, RAM, Ephemeral Disk & Persistent Disk made available, as required.

1.  Click the **Save** button.

####  <a id="dedicated-vm-config"></a>Dedicated-VM Plan

1.  Select the **Resource Config** tab to change the allocation of resources for the Dedicated Node.
![dedicated vm config](dedicated-vm-config.png)

  - The default configuration creates five dedicated nodes (VMs). 
  Each node can run one Redis dedicated-VM instance.
  - You can change the number of dedicated nodes, and configure the size of the 
  persistent and ephemeral disks, and the CPU and RAM for each node.
  - The default VM size is small. It is important that you set the correct 
  VM size to handle anticipated loads. 
  - A Redis dedicated-VM instance is allowed up to 45% of the total system RAM on the VM. You can set this with the `maxmemory` configuration. 
  The app can use 100% of `maxmemory`, that is, up to 45% of the system RAM. 
  - Pivotal recommends the persistent disk be set to 3.5x the amount of system RAM.

1.  Click the **Save** button.

##  <a id="securecomms"></a>Configuring Secure Communication
Redis for PCF v1.7.2 lets the operator turn on/off TLS communications for metrics, via a `Use non-secure communication for metrics` checkbox on the metrics configuration
page in Ops Manager. Configure this checkbox for different versions of PCF as follows:

* **PCF v1.8**: Select this checkbox to send metrics to the
Firehose.

* **PCF v1.9**: Clear this checkbox to send metrics to the Firehose securely, or select it to send metrics insecurely.

* **PCF v1.10 and later**: Clear this checkbox to send metrics to the Firehose and avoid errors.

![Configuring Secure Metrics](Enable-TLS.png)

Setting this checkbox incorrectly can cause a `Cannot generate manifest... unknown property "cf_etcd_client_cert"` error:

![Configuring Secure Metrics error](TLS-error.png)

### <a id="syslog"></a>Configure Syslog Output

Pivotal recommends that operators configure a syslog output.

1.  Add the Syslog address, Syslog port and transport protocol of your log management tool.

    The information required for these fields is provided by your log management tool.

    ![syslog configuration](syslog-forwarding.png)

1.  Click the **Save** button.

### <a id="backup"></a>Creating Backups of Redis Instances

You can configure backups to be run for all instances, across both service plans.

The key features are:

* Runs on a configurable schedule
* Every instance is backed up, across both service plans
* The Redis broker statefile is backed up
* For each backup artefact, a file is created that contains the MD5 checksum for that artifact. This can be used to validate that the artefact is not corrupted.
* You can configure AWS S3, SCP, Azure or Google Cloud Storage as your destination
* Data from Redis is flushed to disk, before the backup is started by running a `BGSAVE` on each instance
* Backups are labelled with timestamp, instance GUID and plan name

#### Configuration
To enable backups, you will first need to choose your backup destination type - AWS S3, SCP, Azure or Google Cloud Storage.

Click on the tile in OpsManager, followed by the `Backups` link on the left hand menu.
![OpsManager Backups view](redis-backup.png)

##### S3 backup fields
![OpsManager Backups S3 view](s3backupoptions.png)

| Field             | Description                                                                                                                                                                                                                                                                                                                                                                                            | Mandatory/Optional                                               |
|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|
| Access Key ID     | The access key for your S3 account                                                                                                                                                                                                                                                                                                                                                                     | Mandatory                                                        |
| Secret Access Key | The Secret Key associated with your Access Key                                                                                                                                                                                                                                                                                                                                                         | Mandatory                                                        |
| Endpoint URL      | The endpoint of your S3 account, e.g. `http://s3.amazonaws.com`                                                                                                                                                                                                                                                                                                                                        | Optional, defaults to `http://s3.amazonaws.com` if not specified |
| Bucket Name       | Name of the bucket you wish the files to be stored in.                                                                                                                                                                                                                                                                                                                                                 | Mandatory                                                        |
| Path              | Path inside the bucket to save backups to.                                                                                                                                                                                                                                                                                                                                                             | Mandatory                                                        |
| Backup timeout    | The amount of time, in seconds, that the backup process will wait for the BGSAVE command to complete on your instance, before transferring the RDB file to your configured destination                                                                                                                                                                                                                 | Mandatory                                                        |
| Cron Schedule     | Backups schedule in crontab format.  For example, once daily at 2am is `* 2 * * *`.  Also accepts a pre-defined schedule: any of `@yearly`, `@monthly`, `@weekly`, `@daily`, `@hourly`, or `@every <time>`, where `<time>` is any supported time string (e.g. `1h30m`). For more information, see [the cron package documentation](https://godoc.org/github.com/robfig/cron#hdr-Predefined_schedules). | Mandatory                                                        |

##### AWS IAM Policy
An AWS IAM policy describes the permissions related to your bucket.
The minimum set of policies required in order to upload the backup files are:

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:ListBucketMultipartUploads",
                "s3:ListMultipartUploadParts",
                "s3:PutObject"
            ],
            "Resource": [
                "arn:aws:s3:::<bucket-name>",
                "arn:aws:s3:::<bucket-name>/*"
            ]
        }
    ]
}
```
Notes:

* Make sure to replace `<bucket-name>` with your correct values.
* `s3:CreateBucket` is only required if the S3 bucket does not exist.
* The additional `s3:CreateBucket` action is also required if the S3 bucket does not exist.


##### SCP backup fields
![OpsManager Backups SCP view](scpbackupoptions.png)

| Field                 | Description                                                                                                                                                               | Mandatory/Optional |
|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|
| Username              | The username to use for transferring backups to the scp server                                                                                                            | Mandatory          |
| Private Key           | The private ssh key of the user configured in `Username`                                                                                                                  | Mandatory          |
| Hostname              | The hostname or IP address of the SCP server                                                                                                                              | Mandatory          |
| Destination Directory | The path in the scp server, where the backups will be transferred                                                                                                         | Mandatory          |
| SCP Port              | The scp port of the scp server                                                                                                                                            | Mandatory          |
| Cron Schedule         | Backups schedule in crontab format. Refer to table for S3 backups for details                                                                                             | Mandatory          |
| Backup timeout        | The amount of time, in seconds, that the backup process will wait for the BGSAVE command to complete on your instance, before transferring the RDB file to the scp server | Mandatory          |

##### GCS backup fields
![OpsManager Backups GCS view](gcsbackupoptions.png)

PCF Redis uses service account credentials to upload backups to Google Cloud Storage. The service account should have `Storage Admin` permissions. Please refer to the [documentation](https://cloud.google.com/storage/docs/authentication#service_accounts) for details on how to set up a GCP service account.

| Field                       | Description                                                                                                                                                                                                                                                                                      | Mandatory/Optional |
|-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|
| Project ID                  | GCP Project ID                                                                                                                                                                                                                                                                                   | Mandatory          |
| Bucket name                 | Name of the bucket you wish the files to be stored in.                                                                                                                                                                                                                                           | Mandatory          |
| Service account private key | The JSON Secret Key associated with your Service Account. See [documentation](https://cloud.google.com/storage/docs/authentication#generating-a-private-key) for details on how to set up service account keys.                                                                                  | Mandatory          |
| Cron Schedule               | Backups schedule in crontab format. For example, once daily at 2am is * 2 * * *. Also accepts a pre-defined schedule: any of @yearly, @monthly, @weekly, @daily, @hourly, or @every , where is any supported time string (e.g. 1h30m). For more information, see the cron package documentation. | Mandatory          |
| Backup timeout              | The amount of time, in seconds, that the backup process will wait for the BGSAVE command to complete on your instance, before transferring the RDB file to your configured destination                                                                                                           | Mandatory          |


##### Azure backup fields
![OpsManager Backups Azure view](azurebackupoptions.png)

| Field                    | Description                                                                                                                                                                                                                                                                                      | Mandatory/Optional |
|--------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|
| Account                  | Account name                                                                                                                                                                                                                                                                                     | Mandatory          |
| Azure Storage Access Key | Azure specific credentials required to write to the Azure container                                                                                                                                                                                                                              | Mandatory          |
| Container name           | Name of the Azure container which will store backup files.                                                                                                                                                                                                                                       | Mandatory          |
| Destination Directory    | Directory where the backup files will be stored within the Azure container.                                                                                                                                                                                                                      | Mandatory          |
| Blob Store Base URL      | URL pointing to Azure resource                                                                                                                                                                                                                                                                   | Optional           |
| Cron Schedule            | Backups schedule in crontab format. For example, once daily at 2am is * 2 * * *. Also accepts a pre-defined schedule: any of @yearly, @monthly, @weekly, @daily, @hourly, or @every , where is any supported time string (e.g. 1h30m). For more information, see the cron package documentation. | Mandatory          |
| Backup timeout           | The amount of time, in seconds, that the backup process will wait for the BGSAVE command to complete on your instance, before transferring the RDB file to your configured destination                                                                                                           | Mandatory          |

###### Notes

For each backup destination, the field `Backup timeout` causes backups to fail after a configured timeout. Redis' BGSAVE will continue but backups will not be uploaded to destinatons if this timeout is hit.


### <a id="azs"></a>Networks, Security, and Assigning AZs

####  <a id="network-configuration"></a>Network Configuration

The following ports and ranges are used in this service:

| Port                  | Protocol                     | Direction and Network                                                                          | Reason                                                                                                                      |
|:----------------------|:-----------------------------|:-----------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|
| 8300<br>8301             | tcp<br>tcp and udp | Inbound to CloudFoundry network, outbound from service broker and service instance networks* | Communication between the CF consul\_server and consul\_agents on Redis deployment; used for metrics                         |
| 4001                  | tcp                          | Inbound to CloudFoundry network, outbound from service broker and service instance networks* | Used by the Redis metron_agent to forward metrics to the CloudFoundry etcd server                                          |
| 80                 | tcp                          | Outbound from CloudFoundry to the cf-redis-broker service broker network                       | (Only if using a cf-redis-broker) Access to the cf-redis-broker from the cloud controllers.                                 |
| 6379                  | tcp                          | Outbound from CloudFoundry to any service instance networks      | Access to all nodes from the Diego Cell and Diego Brain network(s)                            |
| 32768-61000           | tcp                          | Outbound from CloudFoundry to the cf-redis-broker service broker network                       | From the Diego Cell and Diego Brain network(s) to the service broker VM. This is only required for the shared service plan. |
| 80 or 443<br>(Typically) | http or https<br>respectively  | Outbound from any service instance networks                                                    | Access to the backup blobstore                                                                                             |

\* Typically the service broker network and service instance network(s) are the same.

####  <a id="asg"></a> Application Security Groups###

To allow this service to have network access you must create [Application Security Groups (ASGs)](http://docs.pivotal.io/pivotalcf/1-7/adminguide/app-sec-groups.html). Ensure your security group allows access to the Redis Service Broker VM and Dedicated VMs configured in your deployment. You can obtain the IP addresses for these VMs in Ops Manager under the <strong>Resource Config</strong> section for the Redis tile.</p>

<p class="note"><strong>Note</strong>: Without ASGs, this service is unusable.</p>

##### Application Container Network Connections

Application containers that use instances of the Redis service require the following outbound network connections:

<table><thead>
<tr>
<th>Destination</th>
<th>Ports</th>
<th>Protocol</th>
<th>Reason</th>
</tr>
</thead><tbody>
<tr>
<td><code>ASSIGNED_NETWORK</code></td>
<td>32768-61000</td>
<td>tcp</td>
<td>Enable application to access shared vm service instance</td>
</tr>
<tr>
<td><code>ASSIGNED_NETWORK</code></td>
<td>6379</td>
<td>tcp</td>
<td>Enable application to access dedicated vm service instance</td>
</tr>
</tbody></table>

Create an ASG called `redis-app-containers` with the above configuration and bind it to the appropriate space or, to give all started apps access, bind to the `default-running` ASG set and restart your apps. Example:

```json
[
  {
    "protocol": "tcp",
    "destination": "ASSIGNED_NETWORK",
    "ports": "6379"
  }
]
```

####  <a id="az-config"></a>Assigning AZs
Assigning multiple AZs to Redis jobs will not guarantee high availability.

All of your Shared-VM instances will run on a single node in just one of the configured availability zones and are therefore not highly availabile.

Each Dedicated-VM instance could be assigned to any of the configured availability zones, however each instance still operates as a single node with no clustering. This separation over availability zones provides no high availability.

![AZ Assignment Diagram](AZ-assignment.png)

## <a id="validation"></a>Validating Installation

### Smoke tests

Smoke tests are run as part of Redis for PCF installation to validate that the install succeeded. Smoke tests are described <a href="/redis/1-6/smoke-tests.html">here</a>.

## <a id="upgrades"></a>Upgrading Redis for PCF

This product enables a reliable upgrade experience between versions of the product that is deployed through Ops Manager.

The upgrade paths are detailed [here](http://docs.pivotal.io/redis/1-6/index.html) for each released version.

To upgrade the product:

* The Operator should download the latest version of the product from [Pivotal Network](https://network.pivotal.io/products/p-redis)
* Upload the new .pivotal file to Ops Manager
* Upload the stemcell associated with the update (*if required*)
* Update any new mandatory configuration parameters (*if required*)
* Press "Apply changes" and the rest of the process is automated

During the upgrade deployment each Redis instance will experience a small period of downtime as each Redis instance is updated with the new software components. This downtime is because the Redis instances are single VMs operating in a non HA setup. The length of the downtime depends on whether there is a stemcell update to replace the operating system image or whether the existing VM can simply have the redis software updated. Stemcells updates incur additional downtime while the IaaS creates the new VM while updates without a stemcell update are faster.

Ops Manager ensures the instances are updated with the new packages and any configuration changes are applied automatically.

Upgrading to a newer version of the product does not cause any loss of data or configuration. This is explicitly tested for during our build and test process for a new release of the product.

### Release policy

When a new version of Redis is released we aim to release a new version of the product containing this soon after.

Where there is a new version of Redis or another dependent software component such as the stemcell released due to a critical CVE, Pivotal's goal is to release a new version of the product within 48 hours.

## <a id="uninstall"></a>Uninstalling Redis for PCF

To uninstall Redis for PCF, click on the trashcan icon in the lower right hand corner of the PCF Redis tile in the PCF Ops Manager Installation dashboard. Confirm deletion of the product and click apply changes.
