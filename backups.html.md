---
title: Redis for Pivotal Cloud Foundry&reg;
owner: London Services
---

<a id="backup"></a>
# Backups

You can configure backups to be run for each instance, across both service plans.

The key features are:

* Runs at midnight system time every day (not configurable)
* Every instance is backed up, across both service plans
* You can configure an S3 compatible blobstore as your destination
* Data from Redis is flushed to disk, before the backup is started by running a `BGSAVE` on each instance
* Currently certified and tested against AWS S3 only

## Configuration
To enable backups to be taken, you need to configure the mandatory options in the `Redis for PCF` tile in OpsManager.

Click on the tile in OpsManager, followed by the `Backups` link on the left hand menu.

![View of OpsManager Backup configuration](backups.jpeg)

### Access Key ID
This is your Access Key for your Blobstore

**Required?** No - this is optional, dependent upon whether is required by your blobstore

### Secret Access Key
This is your Secret associated with your access key id

**Required?** No - this is optional, dependent upon whether is required by your blobstore

### Endpoint URL
This is the endpoint for your blobstore e.g. `http://s3.amazonaws.com`

**Required?** Yes - If you want to enable backups to be run, you must populate this field.

### Bucket Name
Name of the bucket inside your blobstore you wish the files to be stored in.

**Required?** Yes - If you want to enable backups to be run, you must populate this field.

### Path
Path inside the bucket

**Required?** No - this is optional, it will default to the root if not specified

### Redis BGSAVE Timeout
This is the amount of time that the backup process will wait for the BGSAVE command to complete on your instance, before transferring the RDB file to your configured blobstore.

You can increase this if required for your setup.

**Required?** - Yes, this defaults to 600 seconds.

## AWS IAM Policy
The minimum set of policies required in order to upload the backup files are:

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:CreateBucket",
                "s3:PutObject"
            ],
            "Resource": [
                "arn:aws:s3:::<bucket-name>",
                "arn:aws:s3:::<bucket-name>/*"
            ]
        }
    ]
}
```
Make sure to replace `<bucket-name>` with your correct value.

## Manual Backups

It is possible to create a backup of an instance manually by following these steps:

* [Follow these steps](http://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#ssh) to log into your Ops Manager installation and target the Redis tile deployment.
* Identify the VM which holds your instance by running `bosh vms`.
  * For the `shared-vm` plan this will be the job name containing `cf-redis-broker`.
  * For the `dedicated-vm` plan this will be the job name containing `dedicated-node`.
  * You can identify the exact node for your `dedicated-vm` service instance by comparing the IP Address from your application bindings.

An example output from `bosh vms`:

![OpsManager VMs view](bosh_vms.jpeg)

* Target your redis deployment with `bosh deployment`.
* `bosh ssh` into your desired node.

Persistence is enabled on these plans through the use of `RDB` files, using the following Redis config rules:
```
save 900 1
save 300 10
save 60 10000
```

### Shared-VM Plan

You can either take the latest RDB file held on disk, which is generated by the above the rules, or trigger a recent update by using the `redis-cli` to trigger a `BGSAVE`. Credentials to log into the `redis-cli` can be obtained from `VCAP_SERVICES` for your bound application.

The `redis-cli` is located in `/var/vcap/packages/redis/bin/redis-cli`.

On this plan, the `BGSAVE` command is aliased to a random string. This can be obtained from Ops Manager in the credentials tab.

#### Steps to Backup

* `bosh ssh` into your desired node. See the above section to identify the correct VM.
* Change to Root using `sudo -i`.
* Copy the contents of the `/var/vcap/store/cf-redis-broker` directory to a zip or tar file.
* Backup the folder / compressed file to your chosen location.

The `/var/vcap/store/cf-redis-broker` has sub-directories for each instance created of this plan. The backup file for each instance is called `dump.rdb`.

For example, here are two instances:
```
root@66358f3e-3428-46df-9bb3-9acc7770b188:/var/vcap/store/cf-redis-broker# find -type f | xargs ls -1
./redis-data/3124f373-e9e2-44e1-ad12-a8865d8978b0/db/dump.rdb
./redis-data/3124f373-e9e2-44e1-ad12-a8865d8978b0/redis.conf
./redis-data/3124f373-e9e2-44e1-ad12-a8865d8978b0/redis-server.pid
./redis-data/62333bf9-f023-4566-b233-6686f26b8f4d/db/dump.rdb
./redis-data/62333bf9-f023-4566-b233-6686f26b8f4d/redis.conf
./redis-data/62333bf9-f023-4566-b233-6686f26b8f4d/redis-server.pid
./statefile.json
```

### Dedicated-VM Plan

You can either take the latest RDB file on disk, as generated by the above rules, or trigger a more recent RDB file by executing the `BGSAVE` command using the `redis-cli`. Credentials can be obtained from the `VCAP_SERVICES` from your bound application.
The `redis-cli` can be found in `/var/vcap/packages/redis/bin/redis-cli`.

#### Steps to Backup

* `bosh ssh` into your desired node. See the above section to identify the correct VM.
* Change to Root using `sudo -i`.
* Copy the contents of the `/var/vcap/store/redis` directory to a zip or tar file.
* Backup the folder / compressed file to your chosen location.

The backup file will be named `dump.rdb`.

<a id="restore"></a>
## Restore Locally
You can choose to restore the RDB file to a local Redis instance.

The steps to do this depend on your configuration and setup.
Refer to the [Redis documentation](http://redis.io/documentation) for more details.

## Restore to PCF
You can also restore your backup file to another instance of the `Redis for PCF` tile.

The below steps are manual. These will be replaced by an automated, operator-friendly script in a future release of the product.

Before restoring your RDB file you must have these pre-requisites:

* Same resource configuration as the instance from which you backed up.
  * The persistent disk should be increased to be `3.5 x size of the RDB file` if it is not already so. This allows space for the temporary files used during the restore process

To restore your backup file, to another instance of the `Redis for PCF` tile and the `dedicated-vm` plan.

1. Create a new instance of the plan that you wish to restore to.
1. Identify the VM which the instance of your plan is located on by following the steps from the `Manual Backups` section above.
1. `bosh ssh` into the identified VM.
1. Copy the RDB file you wish to restore to `/var/vcap/store/` as root or using `sudo`.
1. Stop all running `Redis` processed by running `sudo monit stop all`.
1. Identify the location of the master `redis.conf` file with `find /var/vcap/data/jobs/ -name redis.conf`.
  1. For example:  `/var/vcap/data/jobs/dedicated-node/f0ff6d25683b2ab7b14ee0b4b5eb9bd8e9ce8cfc-d59095917f58e170c252500ed7873cb8c359b757/config/redis.conf`
1. Open the file identified in an editor such as `vim`
  1. Modify the following value:
    1. From: `appendonly yes`
    1. To: `appendonly no`
  1. Save the file and exit.
1. For the `dedicated-vm` plan, replace the file `/var/vcap/store/redis/dump.rdb` with the file you copied over in step 4, making sure to retain the filename `dump.rdb`.
1. For the `shared-vm` plan, replace the file `/var/vcap/store/cf-redis-broker/redis-data/<guid>/dump.rdb` with the file you copied over in step 4, making sure to retain the filename `dump.rdb`.
1. Confirm the file size of `dump.rdb` is as you expect by running `ls -alh`.
1. Restart the processes by running `sudo monit start all`.
1. Redis will now load the restored `dump.rdb` file into memory and write it back out to the AOF file.
  1. The amount of time this takes will depend upon your dataset size and other factors.
  1. To view progress, use the `redis-cli` and run the `info` command.
  1. Wait until you see `aof_rewrite_in_progress:0`.
1. Stop the running Redis process with `sudo monit stop all`.
1. Reverse the configuration changes in step 9 so you have `appendonly yes`.
1. Start the processes again with `sudo monit start all`.
1. You have now restored your Redis data.
