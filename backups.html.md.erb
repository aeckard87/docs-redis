---
title: Redis for Pivotal Cloud Foundry
---

# Backups

You can configure backups to be run for each instance, across both service plans. 

The key features are:

* Runs at midnight system time every day (not configurable)
* Every instance is backed up, across both service plans
* You can configure an S3 compatible blobstore as your destination
* Data from Redis is flushed to disk, before the backup is started by running a `BGSAVE` on each instance
* Currently certified and tested against AWS S3 only

## Configuration
To enable backups to be taken, you need to configure the mandatory options in the `Redis for PCF` tile in OpsManager.

Click on the tile in OpsManager, followed by the `Backups` link on the left hand menu. 

<%= image_tag("backups.jpeg") %>

### Access Key ID 
This is your Access Key for your Blobstore

**Required?** No - this is optional, dependent upon whether is required by your blobstore

### Secret Access Key 
This is your Secret associated with your access key id

**Required?** No - this is optional, dependent upon whether is required by your blobstore

### Endpoint URL
This is the endpoint for your blobstore e.g. `http://s3.amazonaws.com`

**Required?** Yes - If you want to enable backups to be run, you must populate this field. 

### Bucket Name
Name of the bucket inside your blobstore you wish the files to be stored in.

**Required?** Yes - If you want to enable backups to be run, you must populate this field. 

### Path
Path inside the bucket

**Required?** No - this is optional, it will default to the root if not specified

### Redis BGSAVE Timeout
This is the amount of time that the backup process will wait for the BGSAVE command to complete on your instance, before transferring the RDB file to your configured blobstore. 

You can increase this if required for your setup. 

**Required?** - Yes, this defaults to 600 seconds. 


## Manual Backups
As well as the scheduled backups, it is possible to create a backup of an instance manually by following these steps

* [Follow these steps to login to your OpsManager installation and target the Redis tile deployment](http://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#ssh)
* Identify the VM which holds your instance by running `bosh vms`
  * For the `shared-vm` plan this will be the job name containing `cf-redis-broker`
  * For the `dedicated-vm` plan this will be the job name containing `dedicated-node`.
  * You can identify the exact node for your `dedicated-vm` service instance by comparing the IP Address from your application bindings

An example output from `bosh vms`
<%= image_tag("bosh_vms.jpeg") %>

* target your redis deployment with `bosh deployment`
* `bosh ssh` into you desired node
* execute these commands, dependent upon the service plan
  * `shared-vm` plan, execute `BROKER_CONFIG_PATH=/var/vcap/jobs/cf-redis-broker/config/backup.yml /var/vcap/packages/cf-redis-broker/bin/backup`
  * `dedicated-vm` plan, execute `BROKER_CONFIG_PATH=/var/vcap/jobs/dedicated-node/config/backup.yml /var/vcap/packages/cf-redis-broker/bin/backup`
  * these commands will complete and provide output as the steps completed, along with any error messages if encountered.
  * the time taken is dependent upon a combination of the data set size and network transfer speed
* check your blobstore to confirm the files were transferred

## Restore Locally
You can choose to restore the RDB file to a local Redis instance if you wish. 

The steps to do this will depend on your configuration and setup.
[Refer to the Redis documentation for more details](http://redis.io/documentation) 

## Restore to PCF
You can also restore your backup file to another instance of the `Redis for PCF` tile. 

The below steps are manual, these will be replaced by an automated operator friendly script in a future release of the product. 

Before restoring your RDB file you must have these pre-requisites:

* Same resource configuration as the instance from which you backed up
  * The persistent disk should be increased to be `3.5 x size of the RDB file` if it not already so.
  * This is to allow space for the temporary files used during the restore process

To restore your backup file, to another instance of the `Redis for PCF` tile

1. Create a new instance of the plan that you wish to restore to
1. Identify the VM which the instance of your plan is located on, by following the steps from the `Manual Backups` section above
1. `bosh ssh` into the identified VM
1. copy the RDB file you wish to restore to `/var/vcap/store/` as root or using `sudo`
1. stop all running `Redis` processed by running `sudo monit stop all`
1. Identify the location of the master `redis.conf` file with `find /var/vcap/data/jobs/dedicated-node -name redis.conf`
  1. For example, `/var/vcap/data/jobs/dedicated-node/f0ff6d25683b2ab7b14ee0b4b5eb9bd8e9ce8cfc-d59095917f58e170c252500ed7873cb8c359b757/config/redis.conf`
1. Open the file identified in an editor such as `vim`
  1. modify the following value:
    1. from: `appendonly yes`
    1. to: `appendonly no`
  1. exit & save
1. replace the file `/var/vcap/store/redis/dump.rdb` with the file you copied over in step 4, making sure to retain the filename `dump.rdb`
1. confirm the file size of `dump.rdb` is as you expect by running `ls -alh`
1. restart the processes by running `sudo monit start all`
1. Redis will now load the restored `dump.rdb` file into memory and write it back out to the AOF file
  1. the amount of time this takes will depend upon your dataset size and other factors
  1. to view progress you can use the `redis-cli` and run the `info` command
  1. wait until you see `aof_rewrite_in_progress:0`
1. stop the running Redis process with `sudo monit stop all`
1. Reverse the configuration changes in step 9 so you have `appendonly yes`
1. start the processes again with `sudo monit start all`
1. you have now restored your Redis data